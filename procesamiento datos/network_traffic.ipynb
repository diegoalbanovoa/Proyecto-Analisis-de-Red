{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project :Network traffic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pandas, matplotlib.pyplot, os and random imports for implementing a Random Forest algorithm. \n",
    "These libraries and modules provide useful functionality for data manipulation, visualization, and random operations.\n",
    "\n",
    "import pandas as pd: Imports the pandas library and assigns it the alias \"pd\". \n",
    "This library is a data analysis that offers flexible and efficient data structures, such as DataFrames, that can be used to store and manipulate the data used in training and evaluating a Random Forest model.\n",
    "\n",
    "import matplotlib.pyplot as plt: Imports the pyplot module from the matplotlib library and assigns it the alias \"plt\". \n",
    "Matplotlib is a Python visualization library, and pyplot provides an interface for creating plots and visualizations. \n",
    "\n",
    "import os: Imports the \"os\" module that provides functions to interact with the operating system. \n",
    "We use it for access to filepaths, create directories, verify the existence of files, among other operations related to the manipulation of files or directories necessary in the development of the algorithm.\n",
    "\n",
    "import random: Imports the \"random\" module that provides functions for generating random numbers and performing random operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join different network traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la ruta de los archivos CSV en crudo\n",
    "raw_data_path = r\"../Dataset en Crudo\"\n",
    "\n",
    "# Busca todos los archivos CSV en la ruta\n",
    "csv_files = [f for f in os.listdir(raw_data_path) if f.endswith('.csv')]\n",
    "\n",
    "# Crea una lista vacía para almacenar los DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Itera sobre cada archivo CSV y agrega su DataFrame a la lista\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(raw_data_path, csv_file)\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatena todos los DataFrames en uno solo\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Define la ruta y el nombre del archivo final\n",
    "output_path = r\"../Dataset Crudo Proceso de Clasificación/output.csv\"\n",
    "\n",
    "# Guarda el archivo final en la ruta especificada\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Archivo final guardado en: \", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code finds all the CSV files in a directory, loads them into separate DataFrames, and then concatenates them into a single DataFrame. Finally, it saves this DataFrame to a specified CSV file.\n",
    "\n",
    "Defines the path of the directory containing the raw CSV files: raw_data_path = r\"../Dataset in Raw\"\n",
    "\n",
    "It uses the os.listdir() function to find all CSV files in the specified directory and stores them in the csv_files list.\n",
    "\n",
    "Creates an empty list called dataframes that will be used to store the DataFrames of the CSV files.\n",
    "\n",
    "Iterates over each CSV file in csv_files. For each file, create the full path using os.path.join() and then use pandas' pd.read_csv() to read the CSV file and load it into a DataFrame called df. Next, add this DataFrame to the dataframes list.\n",
    "\n",
    "Use pandas' pd.concat() to concatenate all the DataFrames in the dataframes list into a single DataFrame called final_df. The ignore_index=True parameter is used to reset the indexes of the resulting DataFrame.\n",
    "\n",
    "Define the path and name of the final file using output_path.\n",
    "\n",
    "Use the pandas to_csv() method on the final_df DataFrame to save the final file to the path specified by output_path . The index=False parameter is used to not include the index in the saved CSV file.\n",
    "\n",
    "Prints the path of the final file saved to the console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data traffic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset and load it into dataframe\n",
    "data = r\"../Dataset Crudo Proceso de Clasificación/output.csv\"\n",
    "df = pd.read_csv(data, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initial scan of the datset analysis, the rows and columns contained in the dataset are counted using the *\"shape\"* command.\n",
    "After this, we start to arrange the dataset, for this the first thing to do is to eliminate the null data that we have, for this we use the command *\"dropna\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset dimension\n",
    "shape = df.shape\n",
    "print(f\"Columns: {shape[1]}, Rows: {shape[0]}\")\n",
    "\n",
    "#Remove null data or clean the dataset of garbage elements\n",
    "df = df.dropna()\n",
    "print(f\"Dataset dimension with no null or bad data \\nColumns: {df.shape[1]}, Rows: {df.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we review what is the information contained in the dataset using *\"info\"*, in this case it contains **585462 data** in 7 columns and these have a division as follows with different data types\n",
    "\n",
    "*   No.          int64  \n",
    "*   Time         float64\n",
    "*   Source       object \n",
    "*   Destination  object\n",
    "*   Protocol     object \n",
    "*   Length       int64  \n",
    "*   Info         object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data frame summary\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of unique variables in that column is obtained from the column called *\"Protocol\"*, for this the ***\".unique()\"*** method is used. Here it gives us how many protocols are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values of the 'Protocol' column in the DataFrame df.\n",
    "print(f\"Values of the 'Protocol' \\n{df['Protocol'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the frequency of each of the variables is calculated, in this case obtaining that the TCP protocol has the highest frequency with ***419540 data***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the frequency of each protocol\n",
    "df['Protocol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After knowing the amount of data, a bar chart is made with all the protocols to make it easier to understand the frequency with which each protocol appears in the dataset.\n",
    "\n",
    "Here it is shown that out of the 65 protocols there are only data in 24 protocols, some of which have more frequency than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bar chart to show the frequency of each protocol in the data set\n",
    "\n",
    "# Calculate the frequency of each protocol\n",
    "protocol_freq = df['Protocol'].value_counts()\n",
    "\n",
    "# Create a bar chart for the protocol frequencies\n",
    "protocol_freq.plot(kind='bar', figsize=(10, 6))\n",
    "plt.xlabel('Protocol')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Protocol Frequencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Data transformstion section we do the same procedure we did initially, that is to define the path of the file to be read and the folder where the new data will be saved, the file is read, and now the *\"No\"* column is eliminated since it is a column that does not represent important information for the analysis to be performed.\n",
    "\n",
    "Then, the folder is created to avoid any inconvenience of non-existence, then the dictionary that will store the DataFrames of each protocol is created.\n",
    "\n",
    "Now we filter the results by the layer 3 protocols that we want to take into account for this network traffic analysis, here we verify that there are enough rows for each protocol since we want to level the amount of data to be analyzed, in this case we left 700 as the amount of data for each protocol.\n",
    "\n",
    "Finally, each dataframe is saved in a separate csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la ruta del archivo CSV y la carpeta para guardar los nuevos archivos\n",
    "csv_path = r'../Dataset Crudo Proceso de Clasificación/output.csv'\n",
    "folder_path = r'../Dataset Proceso de Clasificación'\n",
    "\n",
    "# Lee el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Elimina la columna \"No.\"\n",
    "df = df.drop('No.', axis=1)\n",
    "\n",
    "# Crea la carpeta si no existe\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Crea un diccionario para almacenar los DataFrames de cada protocolo\n",
    "protocol_dfs = {}\n",
    "\n",
    "# Filtra los resultados del conteo por los protocolos de aplicación conocidos\n",
    "app_protocols = ['HTTP', 'DNS', 'TCP', 'FTP', 'ICMP', 'UDP']\n",
    "for protocol in app_protocols:\n",
    "    # Verifica si hay suficientes filas en el DataFrame para el protocolo actual\n",
    "    if (df['Protocol'] == protocol).sum() <= 700:\n",
    "        data = (df['Protocol'] == protocol).sum()\n",
    "        print(data)\n",
    "        print(f\"No hay suficientes filas para el protocolo {protocol}\")\n",
    "        continue\n",
    "    \n",
    "    # Selecciona 700 filas aleatorias del DataFrame para cada protocolo\n",
    "    random.seed(42)\n",
    "    protocol_df = df.loc[df['Protocol'] == protocol].sample(n=700, random_state=42)\n",
    "    protocol_dfs[protocol] = protocol_df\n",
    "\n",
    "# Guarda cada DataFrame en un archivo CSV separado\n",
    "for protocol, protocol_df in protocol_dfs.items():\n",
    "    filename = f'{protocol.lower()}_output.csv'\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    protocol_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Dataset Consolidation section you define the path to the folder that contains each of the csv files that were previously created.\n",
    "\n",
    "Then an empty list is created where the Dataframes of each CSV file will be stored, then each of these files is read and added to the list that was created.\n",
    "\n",
    "Now we concatenate the frames so that they are as one and we make the mixture of the rows in a pseudo-random way.\n",
    "\n",
    "Then this Dateframe mixed in a pseudo-random way is saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la ruta de la carpeta que contiene los archivos CSV\n",
    "folder_path = r'../Dataset Proceso de Clasificación'\n",
    "\n",
    "# Crea una lista vacía para almacenar los DataFrames de cada archivo CSV\n",
    "dfs = []\n",
    "\n",
    "# Lee cada archivo CSV en un DataFrame y agrégalo a la lista\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatena los DataFrames en uno solo\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Mezcla las filas del DataFrame de forma pseudoaleatoria\n",
    "random.seed(42)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Guarda el DataFrame mezclado en un archivo CSV\n",
    "mixed_csv_path = r'../Dataset Analisis de redes/dataset.csv'\n",
    "df.to_csv(mixed_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we count again the amount of data per protocol, here we check that in fact all of them have 700 data to have an equal distribution of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Protocol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These frequencies are then displayed in a bar chart showing the protocol and the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bar chart to show the frequency of each protocol in the data set\n",
    "\n",
    "# Calculate the frequency of each protocol\n",
    "protocol_freq = df['Protocol'].value_counts()\n",
    "\n",
    "# Create a bar chart for the protocol frequencies\n",
    "protocol_freq.plot(kind='bar', figsize=(10, 6))\n",
    "plt.xlabel('Protocol')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Protocol Frequencies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box plot (or box and whisker plot) of the packet lengths in a data set is now created. The box plot is a graphical tool used to represent the distribution of a data set and can be useful for visualizing the distribution of data and detecting outliers in a data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot of packet lengths\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(df['Length'])\n",
    "plt.xlabel('Packet Length')\n",
    "plt.ylabel('Length')\n",
    "plt.title('Distribution of Packet Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using a *\"for\"* to create bar charts of the source and destination IP address frequencies for different network protocols. \n",
    "\n",
    "First a list called protocols is created containing the names of the network protocols to be analyzed. Then, a for loop is started that loops through each protocol in the protocols list, after which the DataFrame df is filtered to obtain only the rows that correspond to the current protocol in the for loop and the frequencies of the destination IP addresses are obtained in the filtered DataFrame protocol_df.\n",
    "Now with *\"source_freq = protocol_df['Source'].value_counts()\"* used to get the frequencies of the source IP addresses in the filtered DataFrame.\n",
    "Finally, the bar charts of the source and destination IP address frequencies for the network protocols are created. For each protocol, the DataFrame df is filtered to obtain only the rows that correspond to the protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols = ['HTTP', 'DNS', 'TCP', 'FTP', 'ICMP', 'UDP']\n",
    "\n",
    "for protocol in protocols:\n",
    "    # Filter the DataFrame for the specific protocol\n",
    "    protocol_df = df[df['Protocol'] == protocol]\n",
    "    \n",
    "    # Get the frequency of the destination IP addresses\n",
    "    destination_freq = protocol_df['Destination'].value_counts()\n",
    "\n",
    "    # Get the frequency of the source IP addresses\n",
    "    source_freq = protocol_df['Source'].value_counts()\n",
    "\n",
    "    # Create a bar chart for the destination IP address frequencies\n",
    "    destination_freq.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.xlabel('Destination IP Address')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{protocol} - Destination IP Address Frequencies')\n",
    "    plt.show()\n",
    "\n",
    "    # Create a bar chart for the source IP address frequencies\n",
    "    source_freq.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.xlabel('Source IP Address')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{protocol} - Source IP Address Frequencies')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
